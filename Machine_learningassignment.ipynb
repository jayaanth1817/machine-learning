{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9C6Id6MT5x3",
        "outputId": "827a604e-7b91-4da2-b4f1-0fdebed167ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/machinelearning-assingment2-dataset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Income Binning: Divide the 'Income' column into bins (low, medium, high)\n",
        "data['Income_Bin'] = pd.cut(data['Income'], bins=[0, 30000, 70000, float('inf')], labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "# Drop the original 'Income' column after binning\n",
        "data = data.drop(columns=['Income'])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['Fraud', 'Credit_card_number', 'Expiry', 'Security_code'])\n",
        "y = data['Fraud']\n",
        "\n",
        "# Define the categorical columns to be encoded\n",
        "categorical_cols = ['Profession', 'Income_Bin']\n",
        "\n",
        "# Preprocess categorical data using OneHotEncoding and scale numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_cols),\n",
        "        ('num', StandardScaler(), X.select_dtypes(include=['int64', 'float64']).columns)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep other columns as is\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"SVM\": SVC(probability=True)  # Enable probability estimation for AUC calculation\n",
        "}\n",
        "\n",
        "# Define hyperparameters for each model\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\n",
        "        'max_depth': [5, 10, 15],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [5, 10, 15],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [5, 10, 15],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        'C': [0.1, 1, 10],  # Regularization parameter\n",
        "        'kernel': ['linear', 'rbf'],  # Kernel types\n",
        "        'gamma': ['scale', 'auto']  # Kernel coefficient\n",
        "    }\n",
        "}\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Choose the scoring metric\n",
        "scoring_metric = 'f1'\n",
        "\n",
        "# Perform hyperparameter tuning with GridSearchCV using pipelines\n",
        "best_models = {}\n",
        "\n",
        "for model_name in models:\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "    # Create a pipeline with preprocessing and the model\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        (model_name, models[model_name])\n",
        "    ])\n",
        "\n",
        "    # Construct parameter grid with correct prefix for the model in the pipeline\n",
        "    param_grid = {f'{model_name}__' + key: value for key, value in param_grids[model_name].items()}\n",
        "\n",
        "    # Perform GridSearchCV with the chosen scoring metric\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring=scoring_metric, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Save the best model\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "\n",
        "    # Print best parameters and evaluate the model on the test set\n",
        "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
        "    y_pred = best_models[model_name].predict(X_test)\n",
        "\n",
        "    # Evaluate using the chosen metric\n",
        "    if scoring_metric == 'f1':\n",
        "        print(f\"Classification Report for {model_name}:\\n{classification_report(y_test, y_pred)}\")\n",
        "    elif scoring_metric == 'roc_auc':\n",
        "        # For AUC, we need probability estimates\n",
        "        if hasattr(best_models[model_name], \"predict_proba\"):\n",
        "            y_pred_proba = best_models[model_name].predict_proba(X_test)[:, 1]\n",
        "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "            print(f\"AUC-ROC Score for {model_name}: {auc_score}\")\n",
        "        else:\n",
        "            print(f\"{model_name} does not support probability estimates for AUC-ROC calculation.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5jXhAYSZXqG",
        "outputId": "259927a8-4b17-43a8-9a9a-50be000fc0ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Decision Tree...\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best parameters for Decision Tree: {'Decision Tree__max_depth': 5, 'Decision Tree__min_samples_leaf': 1, 'Decision Tree__min_samples_split': 2}\n",
            "Classification Report for Decision Tree:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.48      0.49      1476\n",
            "           1       0.52      0.54      0.53      1524\n",
            "\n",
            "    accuracy                           0.51      3000\n",
            "   macro avg       0.51      0.51      0.51      3000\n",
            "weighted avg       0.51      0.51      0.51      3000\n",
            "\n",
            "\n",
            "Training Random Forest...\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best parameters for Random Forest: {'Random Forest__max_depth': 5, 'Random Forest__min_samples_leaf': 1, 'Random Forest__n_estimators': 50}\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.48      0.49      1476\n",
            "           1       0.52      0.54      0.53      1524\n",
            "\n",
            "    accuracy                           0.51      3000\n",
            "   macro avg       0.51      0.51      0.51      3000\n",
            "weighted avg       0.51      0.51      0.51      3000\n",
            "\n",
            "\n",
            "Training XGBoost...\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [11:19:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for XGBoost: {'XGBoost__learning_rate': 0.01, 'XGBoost__max_depth': 5, 'XGBoost__n_estimators': 100}\n",
            "Classification Report for XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.48      0.49      1476\n",
            "           1       0.52      0.54      0.53      1524\n",
            "\n",
            "    accuracy                           0.51      3000\n",
            "   macro avg       0.51      0.51      0.51      3000\n",
            "weighted avg       0.51      0.51      0.51      3000\n",
            "\n",
            "\n",
            "Training SVM...\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best parameters for SVM: {'SVM__C': 0.1, 'SVM__gamma': 'auto', 'SVM__kernel': 'rbf'}\n",
            "Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.49      0.50      1476\n",
            "           1       0.52      0.53      0.52      1524\n",
            "\n",
            "    accuracy                           0.51      3000\n",
            "   macro avg       0.51      0.51      0.51      3000\n",
            "weighted avg       0.51      0.51      0.51      3000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}